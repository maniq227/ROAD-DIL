#!/usr/bin/env python3
"""
Evaluate existing ER / NAIVE / GDUMB checkpoints on unseen per-domain test YAMLs
generated by build_unseen_tests.py, and export metrics.

Outputs:
- Writes standard Ultralytics results to strategy-specific runs under each project
- Aggregates per-domain metrics to a summary CSV/JSON in Test Set/metrics/

Usage:
    python3 "Test Set/eval_unseen_tests.py" --imgsz 640 --batch 16
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional

from ultralytics import YOLO


BASE_DIR = Path("/Users/muhammadaniq/Documents/Dissertation/ROAD-DIL").resolve()
TEST_DIR = BASE_DIR / "Test Set"
YAMLS_DIR = TEST_DIR / "yamls"
METRICS_DIR = TEST_DIR / "metrics"

DOMAINS = ["sunny", "overcast", "night", "snowy"]


def load_yaml_paths() -> Dict[str, Path]:
    paths: Dict[str, Path] = {}
    for d in DOMAINS:
        p = YAMLS_DIR / f"data_testonly_{d}.yaml"
        if not p.exists():
            raise FileNotFoundError(f"Missing YAML for domain '{d}': {p}")
        paths[d] = p
    return paths


def safe_val(model_path: Path, data_yaml: Path, imgsz: int, batch: int, workers: int) -> Dict[str, Optional[float]]:
    model = YOLO(str(model_path))
    res = model.val(data=str(data_yaml), split="val", imgsz=int(imgsz), batch=int(batch), workers=int(workers))
    out: Dict[str, Optional[float]] = {}
    d = getattr(res, "results_dict", {}) if res is not None else {}
    # Common keys in Ultralytics 8.x
    for k in [
        "metrics/mAP50-95(B)",
        "metrics/mAP50(B)",
        "metrics/precision(B)",
        "metrics/recall(B)",
    ]:
        v = d.get(k)
        out[k] = float(v) if v is not None else None
    return out


def eval_strategy(name: str, checkpoints: Dict[str, Path], yaml_by_domain: Dict[str, Path], imgsz: int, batch: int, workers: int) -> Dict[str, Dict[str, Optional[float]]]:
    rows: Dict[str, Dict[str, Optional[float]]] = {}
    for domain, model_path in checkpoints.items():
        if not model_path.exists():
            continue
        metrics = safe_val(model_path, yaml_by_domain[domain], imgsz, batch, workers)
        rows[domain] = metrics
    return rows


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--imgsz", type=int, default=640)
    ap.add_argument("--batch", type=int, default=16)
    ap.add_argument("--workers", type=int, default=0)
    args = ap.parse_args()

    yaml_by_domain = load_yaml_paths()
    METRICS_DIR.mkdir(parents=True, exist_ok=True)

    # Collect checkpoints (adjust if filenames differ)
    er_ckpts: Dict[str, Path] = {
        "sunny":    BASE_DIR / "YOLO-ER/optimal_output/models/exp1_sunny_best.pt",
        "overcast": BASE_DIR / "YOLO-ER/optimal_output/models/exp2_overcast_best.pt",
        "night":    BASE_DIR / "YOLO-ER/optimal_output/models/exp3_night_best.pt",
        "snowy":    BASE_DIR / "YOLO-ER/optimal_output/models/exp4_snowy_best.pt",
    }
    naive_ckpts: Dict[str, Path] = {
        "sunny":    BASE_DIR / "YOLO-NAIVE/optimal_output/models/exp1_sunny_best.pt",
        "overcast": BASE_DIR / "YOLO-NAIVE/optimal_output/models/exp2_overcast_best.pt",
        "night":    BASE_DIR / "YOLO-NAIVE/optimal_output/models/exp3_night_best.pt",
        "snowy":    BASE_DIR / "YOLO-NAIVE/optimal_output/models/exp4_snowy_best.pt",
    }
    gdumb_ckpts: Dict[str, Path] = {
        "sunny":    BASE_DIR / "YOLO-GDUMB/optimal_output/models/gdumb_model_after_exp_1.pt",
        "overcast": BASE_DIR / "YOLO-GDUMB/optimal_output/models/gdumb_model_after_exp_2.pt",
        "night":    BASE_DIR / "YOLO-GDUMB/optimal_output/models/gdumb_model_after_exp_3.pt",
        "snowy":    BASE_DIR / "YOLO-GDUMB/optimal_output/models/gdumb_model_after_exp_4.pt",
    }

    summary: Dict[str, Dict[str, Dict[str, Optional[float]]]] = {}
    summary["ER"] = eval_strategy("ER", er_ckpts, yaml_by_domain, args.imgsz, args.batch, args.workers)
    summary["NAIVE"] = eval_strategy("NAIVE", naive_ckpts, yaml_by_domain, args.imgsz, args.batch, args.workers)
    summary["GDUMB"] = eval_strategy("GDUMB", gdumb_ckpts, yaml_by_domain, args.imgsz, args.batch, args.workers)

    out_json = METRICS_DIR / "unseen_test_metrics.json"
    out_json.write_text(json.dumps(summary, indent=2), encoding="utf-8")
    # Also write a compact CSV-like summary for quick viewing
    header = ["strategy", "domain", "mAP50-95", "mAP50", "precision", "recall"]
    lines: List[str] = [",".join(header)]
    for strat, rows in summary.items():
        for domain in DOMAINS:
            r = rows.get(domain, {})
            m95 = r.get("metrics/mAP50-95(B)")
            m50 = r.get("metrics/mAP50(B)")
            p = r.get("metrics/precision(B)")
            rc = r.get("metrics/recall(B)")
            def fmt(x: Optional[float]) -> str:
                return "" if x is None else f"{x:.6f}"
            lines.append(
                ",".join([strat, domain, fmt(m95), fmt(m50), fmt(p), fmt(rc)])
            )
    (METRICS_DIR / "unseen_test_metrics.csv").write_text("\n".join(lines) + "\n", encoding="utf-8")

    print(f"[OK] Wrote metrics to: {out_json} and {METRICS_DIR / 'unseen_test_metrics.csv'}")


if __name__ == "__main__":
    main()


